# Sentimental Analysis

import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator

# downloads sample twitter dataset.
nltk.download('twitter_samples')

# We can load the text fields of the positive and negative tweets by using the module's strings() method like this:
# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# Next, we'll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets
print('Number of positive tweets: ', len(all_positive_tweets))
print('Number of negative tweets: ', len(all_negative_tweets))

print('\nThe type of all_positive_tweets is: ', type(all_positive_tweets))
print('The type of a tweet entry is: ', type(all_negative_tweets[0]))

fig = plt.figure(figsize=(5, 5))                                  # Declare a figure with a custom size
labels = 'Positives', 'Negative'                                  # labels for the two classes
sizes = [len(all_positive_tweets), len(all_negative_tweets)]      # Sizes for each slide

# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:
plt.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)

plt.axis('equal')                                                 # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()                                                        # Display the chart

## Looking at raw texts
print('\033[92m' + all_positive_tweets[random.randint(0,5000)])   # print positive in greeen
print('\033[91m' + all_negative_tweets[random.randint(0,5000)])   # print negative in red

## Preprocess raw text for Sentiment analysis
Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:
-Tokenizing the string
-Lowercasing
-Removing stop words and punctuation
-Stemming

# Our selected sample. Complex enough to exemplify each step
tweet = all_positive_tweets[2277]
print(tweet)

nltk.download('stopwords')                 # download the stopwords from NLTK
import re                                  # library for regular expression operations
import string                              # for string operations
from nltk.corpus import stopwords          # module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

## Remove hyperlinks, Twitter marks and styles
Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. 
We'll use the re library to perform regular expression operations on our tweet. 
We'll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')

print('\033[92m' + tweet)
print('\033[94m')
tweet2 = re.sub(r'^RT[\s]+', '', tweet)             # remove old style retweet text "RT"
tweet2 = re.sub(r'https?://[^\s\n\r]+', '', tweet2) # remove hyperlinks
tweet2 = re.sub(r'#', '', tweet2) # remove hashtags # only removing the hash # sign from the word
print(tweet2)

## Tokenize the string
To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. 
The tokenize module from NLTK allows us to do these easily:

print()
print('\033[92m' + tweet2)
print('\033[94m')
# instantiate tokenizer class
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
# tokenize tweets
tweet_tokens = tokenizer.tokenize(tweet2)
print()
print('Tokenized string:')
print(tweet_tokens)

## Remove stop words and punctuations
# Import the english stop words list from NLTK
stopwords_english = stopwords.words('english') 
print('Stop words\n')
print(stopwords_english)

print('\nPunctuation\n')
print(string.punctuation)

We can see that the stop words list above contains some words that could be important in some contexts. 
These could be words like i, not, between, because, won, against. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.
For the punctuation, we saw earlier that certain groupings like ':)' and '...' should be retained when dealing with tweets because they are used to express emotions. 
In other contexts, like medical analysis, these should also be removed.
Time to clean up our tokenized tweet!

print()
print('\033[92m')
print(tweet_tokens)
print('\033[94m')

tweets_clean = []

for word in tweet_tokens: # Go through every word in your tokens list
    if (word not in stopwords_english and  # remove stopwords
        word not in string.punctuation):  # remove punctuation
        tweets_clean.append(word)

print('removed stop words and punctuation:')
print(tweets_clean)

## Stemming
Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary. Consider the words:
-learn
-learning
-learned
-learnt

print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')

# Instantiate stemming class
stemmer = PorterStemmer() 

# Create an empty list to store the stems
tweets_stem = [] 

for word in tweets_clean:
    stem_word = stemmer.stem(word)  # stemming word
    tweets_stem.append(stem_word)  # append to the list

print('stemmed words:')
print(tweets_stem)

## process_tweet()
As shown above, preprocessing consists of multiple steps before you arrive at the final list of words. You will use the function process_tweet(tweet) available in utils.py. 
Open the file and see that this function's implementation is very similar to the steps above.
To obtain the same result as in the previous code cells, you will only need to call the function process_tweet(). Let's do that in the next cell.

from utils import process_tweet # Import the process_tweet function

# choose the same tweet
tweet = all_positive_tweets[2277]

print()
print('\033[92m')
print(tweet)
print('\033[94m')

# call the imported function
tweets_stem = process_tweet(tweet); # Preprocess a given tweet

print('preprocessed tweet:')
print(tweets_stem) # Print the result
