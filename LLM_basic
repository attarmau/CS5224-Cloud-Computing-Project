from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

_new_max_tokens = 32 if not torch.cuda.is_available() else 128

# If you are using Apple M1, M2, M3 on your local machine
# if not torch.cuda.is_available() and torch.backends.mps.is_available():
#   if torch.backends.mps.is_built():
#       device = "mpu"

model_name = "Qwen/Qwen1.5-0.5B"
# if you want a higher degree of challenge, use the following models:
# model_name = "gpt2"
# model_name = "gpt2-medium"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto").to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def complete(primer, max_new_tokens=_new_max_tokens):
  inputs = tokenizer(primer, return_tensors="pt").to(device)

  outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)

  generated_ids = outputs[0][len(inputs.input_ids[0]):]

  return primer, tokenizer.decode(generated_ids, skip_special_tokens=True)

## Complete the sentence (basic function in LLM)
prompt = "Once upon a time, in a quiet village, a young girl discovered a hidden world in her backyard. She was exploring the dense bushes when she stumbled upon a"

primer, completion = complete(prompt)

print("START >>\n\n", primer, sep="")
print()
print("COMPLETION >>\n\n", completion, sep="")

## Prompt for summarization
prompt = \
f"""Please summarize the following text:

{text_to_summarize}

In summary, the key points are:
"""

try:
    primer, completion = complete(prompt)

    print("START >>\n\n", primer, sep="")
    print()
    print("COMPLETION >>\n\n", completion, sep="")
