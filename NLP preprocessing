# Sentimental Analysis

import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator

# downloads sample twitter dataset.
nltk.download('twitter_samples')

# We can load the text fields of the positive and negative tweets by using the module's strings() method like this:
# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# Next, we'll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets
print('Number of positive tweets: ', len(all_positive_tweets))
print('Number of negative tweets: ', len(all_negative_tweets))

print('\nThe type of all_positive_tweets is: ', type(all_positive_tweets))
print('The type of a tweet entry is: ', type(all_negative_tweets[0]))

fig = plt.figure(figsize=(5, 5))                                  # Declare a figure with a custom size
labels = 'Positives', 'Negative'                                  # labels for the two classes
sizes = [len(all_positive_tweets), len(all_negative_tweets)]      # Sizes for each slide

# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:
plt.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)

plt.axis('equal')                                                 # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()                                                        # Display the chart

## Looking at raw texts
print('\033[92m' + all_positive_tweets[random.randint(0,5000)])   # print positive in greeen
print('\033[91m' + all_negative_tweets[random.randint(0,5000)])   # print negative in red

## Preprocess raw text for Sentiment analysis
Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:
-Tokenizing the string
-Lowercasing
-Removing stop words and punctuation
-Stemming

# Our selected sample. Complex enough to exemplify each step
tweet = all_positive_tweets[2277]
print(tweet)

nltk.download('stopwords')                 # download the stopwords from NLTK
import re                                  # library for regular expression operations
import string                              # for string operations
from nltk.corpus import stopwords          # module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

## Remove hyperlinks, Twitter marks and styles
Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the re library to perform regular expression operations on our tweet. We'll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')

print('\033[92m' + tweet)
print('\033[94m')
tweet2 = re.sub(r'^RT[\s]+', '', tweet)             # remove old style retweet text "RT"
tweet2 = re.sub(r'https?://[^\s\n\r]+', '', tweet2) # remove hyperlinks
tweet2 = re.sub(r'#', '', tweet2) # remove hashtags # only removing the hash # sign from the word
print(tweet2)
